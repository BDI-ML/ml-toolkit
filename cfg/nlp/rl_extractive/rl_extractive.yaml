general: 
    seed: 6650
    logdir_base: "{{project_root}}/tensorboard"
    note: "test"

model:
    name: "rl_extractive"
    ckpt_dir: "{{project_root}}/checkpoints"
    save_checkpoint: True
    evaluate: True
    keep_higher_eval: True
    budget: .3
    device: "cuda:2"
    reward_device: "cuda:3"
    #load_checkpoint: "/data/john/projects/mltoolkit/checkpoints/20230519-064232-autolm/best_model.pth"

    # MLDecoder params
    w_rnn_units: 768
    dec_rnn_units: 256
    y_em_size: 256
    decd_drop: .1
    output_features: 2

    # RLModel params
    permute_prob: 0.0
    n_docs: 2
    freeze_base: True
    base_model: "distilbert"
    base_model_ckpt: "{{project_root}}/data/pretrained_models/bert/distilbert_ext.pt"

    # reward model params
    reward_model: 'all-mpnet-base-v1'

optim:
    #lr: 3.0e-5
    lr: 7.0e-7
    weight_decay: 5.0e-6
    beta1: .9
    beta2: .999
    eps: 1.0e-8
    scheduler_freq: 1000
    clip_max_norm: 1
    sched_step_size: 10
    sched_gamma: 0.8

data: 
    loc: "{{project_root}}/data/all_sides/all_sides.csv"
    #num_proc: 24
    num_proc: 0
    pin_memory: True
    num_shards: 1
    cache_dir: "{{home}}/.cache/huggingface"
    shuffle: True
    batch_size: 64
    num_epochs: 50
    eval_freq: 10
    log_freq: 10
    train_test_split: .85
    tokenizer_name: "bert-base-uncased"
    commute_prob: 0.5
    max_seq_len: 512
    n_docs: 2
